This project implements an end-to-end Sign Language to Text system using computer vision and deep learning. The goal of the system is to translate sign language gestures, captured either through uploaded videos or a live webcam feed, into readable English text. The project focuses on word-level sign recognition and demonstrates how machine learning can help reduce communication barriers for sign language users.

The system is built using MediaPipe for hand landmark detection, OpenCV for video and webcam processing, PyTorch for deep learning model training, and FastAPI for exposing the trained model as an API. MediaPipe detects 21 hand landmarks for each video frame, extracting three-dimensional coordinates that represent hand movements. These landmark sequences are then used as input to a deep learning model.

To handle the temporal nature of sign language, an LSTM (Long Short-Term Memory) neural network is used. Each video is converted into a fixed-length sequence of 60 frames, where every frame contains 21 landmarks with x, y, and z coordinates. This allows the LSTM to learn motion patterns over time rather than relying on a single frame. The model outputs a probability distribution over sign classes, and the class with the highest probability is returned as the predicted word.

The dataset used in this project is derived from the WLASL (Word-Level American Sign Language) dataset. A smaller subset of sign videos was selected for experimentation. Each video was preprocessed by extracting hand landmarks, normalizing sequence length, and storing the data in NumPy format for efficient training. This preprocessing step significantly reduced computational overhead compared to training directly on raw video frames.

A FastAPI-based inference service was built to make the model accessible through a REST API. Users can upload an .mp4 video to the /predict endpoint, where the system extracts landmarks, runs the trained LSTM model, and returns the predicted English word along with a confidence score. The API includes Swagger documentation (/docs) for easy testing and interaction. Absolute file paths are used throughout the API to ensure reliability across operating systems, especially on Windows.

In addition to video uploads, a real-time webcam recognition system was implemented using OpenCV. This module continuously captures frames from the webcam, extracts hand landmarks, maintains a rolling buffer of the last 60 frames, and performs live prediction once enough frames are collected. The predicted word and confidence score are displayed directly on the video feed, enabling real-time sign language recognition.

Several challenges were encountered during development. MediaPipe caused installation and environment conflicts on Windows due to multiple Python interpreters, which were resolved by carefully managing the Python environment. Relative file paths caused runtime errors in the FastAPI application when running with Uvicorn’s reload mode; this was fixed by switching to absolute paths using the script’s location. Another issue was the API hanging when processing long or complex videos, which was addressed by limiting the number of frames processed per request.

The initial model accuracy was relatively low, averaging around 30–35%, which is expected given the small dataset size, limited number of training samples per class, and lack of data augmentation. Despite this, the model performs better than random guessing and serves as a solid baseline. The main objective of this project was to validate the complete ML pipeline, rather than maximize accuracy at an early stage.

There are several clear paths for future improvement. Model performance can be enhanced by increasing the dataset size, applying data augmentation techniques, normalizing landmarks, and using more advanced architectures such as bidirectional LSTMs or Transformers. At the system level, the project can be extended to sentence-level translation, grammar correction, and multilingual output. Additional improvements include building a frontend interface, deploying the API to the cloud, and adding accessibility features such as text-to-speech output.

Overall, this project demonstrates a production-style machine learning workflow, covering data preprocessing, model training, real-time inference, API deployment, and system debugging. It is designed to be portfolio-ready and showcases practical skills in computer vision, deep learning, and backend engineering.
